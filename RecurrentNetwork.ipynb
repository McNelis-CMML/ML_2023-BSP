{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c54a05",
   "metadata": {},
   "source": [
    "$\\textbf{Recurrent Neural Networks}$\n",
    "\n",
    "\n",
    "\n",
    "$\\textit{Elman Network}$\n",
    "\n",
    "The Elman Network has memory neurons, instead of using lags of the input variables.  It is like the Moving Average process in traditional econometrics.\n",
    "\n",
    "Here is the setup:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\text{Hidden State (}h_t\\text{):} \\quad & h_t = \\sigma(W_h \\cdot h_{t-1} + W_x \\cdot x_t + b_h)\n",
    "\\end{align*}$\n",
    "\n",
    "$\\textit{Long Short-Term Memory Network}$\n",
    "\n",
    "The LSTM (Long Short-Term Memory) network is a type of recurrent neural network (RNN) that is often used for time series forecasting. LSTM networks are designed to capture long-term dependencies and patterns in sequential data. Here is a set of equations that describe the LSTM network for time series forecasting:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\text{Input Gate (}i_t\\text{):} \\quad & i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\\n",
    "\\text{Forget Gate (}f_t\\text{):} \\quad & f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\\\\n",
    "\\text{Output Gate (}o_t\\text{):} \\quad & o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\\\\n",
    "\\text{Cell State (}C_t\\text{):} \\quad & C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c) \\\\\n",
    "\\text{Hidden State (}h_t\\text{):} \\quad & h_t = o_t \\cdot \\tanh(C_t)\n",
    "\\end{align*}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $i_t$ is the input gate's output, controlling how much new information should be added to the cell state.\n",
    "- $f_t$ is the forget gate's output, controlling how much of the previous cell state should be forgotten.\n",
    "- $o_t$ is the output gate's output, determining the output based on the cell state.\n",
    "- $C_t$ is the updated cell state.\n",
    "- $h_t$ is the hidden state, which is also the output of the LSTM at time step t.\n",
    "- $x_t$ is the input at time step t.\n",
    "- $W_i, W_f, W_o,  W_c$ are weight matrices for the input gate, forget gate, output gate, and cell state, respectively.\n",
    "- $b_i, b_f, b_o, b_c$  are bias vectors for the input gate, forget gate, output gate, and cell state, respectively.\n",
    "- $[h_(t-1), x_t]$ denotes the concatenation of the previous hidden state and the current input.\n",
    "\n",
    "- sigmoid is the sigmoid activation function, and tanh is the hyperbolic tangent activation function.\n",
    "- To use this LSTM network for time series forecasting, you would typically provide historical time series data as input $x_t$ \n",
    "- Ttrain the network to predict future time steps. \n",
    "- During training, the network learns to adjust its parameters $(W_i, W_f, W_o, W_c, b_i, b_f, b_o, b_c)$ to minimize the prediction error. \n",
    "- Once trained, you can use the network to make forecasts by providing a sequence of historical data and letting the LSTM predict the future values.\n",
    "\n",
    "\n",
    "$\\textit{Gated Recurrent Network}$\n",
    "\n",
    "The GRU has the following setup:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\text{Update Gate (}z_t\\text{):} \\quad & z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t]) \\\\\n",
    "\\text{Reset Gate (}r_t\\text{):} \\quad & r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t]) \\\\\n",
    "\\text{Candidate Hidden State (}h_t'\\text{):} \\quad & h_t' = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t]) \\\\\n",
    "\\text{Hidden State (}h_t\\text{):} \\quad & h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot h_t'\n",
    "\\end{align*}$\n",
    "\n",
    " \n",
    "\n",
    "- $z_t$  is the update gate \n",
    "- $r_t$ is the reset gate\n",
    "- $h_t'$ is the candidate hidden state\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $\\odot$  represents element-wise multiplication\n",
    "\n",
    "- The GRU is another type of RNN that combines elements of the LSTM and the Elman RNN. \n",
    "- It has two gates, an update gate and a reset gate, and a hidden state:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d088fa",
   "metadata": {},
   "source": [
    "$\\textit{Elman Network}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc02d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 3ms/step - loss: 0.5405\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3060\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1854\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1241\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1068\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1009\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0978\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0956\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0940\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0927\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0917\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0908\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0901\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0894\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0889\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0884\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0876\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0873\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0870\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0867\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0864\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0862\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0860\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0857\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0856\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0854\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0852\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0851\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0850\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0848\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0846\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0845\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0844\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0843\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0842\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0842\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0841\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0840\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0839\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0839\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0838\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0837\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0837\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0836\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0836\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0835\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0835\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0835\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0834\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0834\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0834\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0832\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0832\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0827\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0827\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Predictions: [[0.47290874]\n",
      " [0.5069924 ]\n",
      " [0.46464562]\n",
      " [0.4889089 ]\n",
      " [0.49753523]\n",
      " [0.48815894]\n",
      " [0.50883615]\n",
      " [0.48265433]\n",
      " [0.48934615]\n",
      " [0.49203157]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Generate random data for training\n",
    "n_samples = 1000\n",
    "n_timesteps = 11\n",
    "n_features = 4\n",
    "\n",
    "X = np.random.rand(n_samples, n_timesteps, n_features)\n",
    "y = np.random.rand(n_samples, n_timesteps)\n",
    "\n",
    "# Create a GRU network\n",
    "model = Sequential()\n",
    "#model.add(GRU(4, activation='tanh', input_shape=(n_timesteps, n_features)))\n",
    "#model.add(Dense(1))  # Output layer\n",
    "# Create an Elman Recurrent Network\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=4, activation='tanh', input_shape=(n_timesteps, n_features), return_sequences=False))\n",
    "model.add(Dense(units=1))  # Output layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, batch_size=32)\n",
    "\n",
    "# Generate new data for prediction (you can replace this with your own test data)\n",
    "X_test = np.random.rand(10, n_timesteps, n_features)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7439c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14f4bf",
   "metadata": {},
   "source": [
    "$\\textit{LSTM Network}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "762be9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 3s 1ms/step - loss: 0.1118\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0907\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0900\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0893\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0891\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0890\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0890\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0890\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0887\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0885\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0889\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0889\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0888\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0887\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0886\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0889\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0885\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0886\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0888\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0886\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0889\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0885\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0884\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0887\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0882\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0886\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0888\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0885\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0887\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0884\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0888\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0884\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0883\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0880\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0881\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0879\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0886\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0887\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0882\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0884\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0885\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0880\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0885\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0885\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0885\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0885\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0884\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0883\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0884\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0879\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0885\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0883\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0884\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0884\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0884\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0880\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0875\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0884\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0877\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0880\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0878\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0883\n",
      "1/1 [==============================] - 0s 455ms/step\n",
      "Predicted Values for the test data:\n",
      "[[0.3741139 ]\n",
      " [0.42571205]\n",
      " [0.50168145]\n",
      " [0.5089371 ]\n",
      " [0.5051675 ]\n",
      " [0.48736456]\n",
      " [0.48349017]\n",
      " [0.47100073]\n",
      " [0.4787231 ]\n",
      " [0.4876212 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Generate random data\n",
    "n_samples = 1000\n",
    "n_timesteps = 1\n",
    "n_features = 4\n",
    "\n",
    "X = np.random.rand(n_samples, n_timesteps, n_features)\n",
    "y = np.random.rand(n_samples, n_timesteps)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, activation='tanh', input_shape=(n_timesteps, n_features)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, batch_size=1)\n",
    "\n",
    "# Generate new data for prediction (1 sample with 10 time steps and 4 features)\n",
    "X_test = np.random.rand(10, n_timesteps, n_features)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Predicted Values for the test data:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d97b2",
   "metadata": {},
   "source": [
    "$\\textit{Gated Recurrent Unit}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c43134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 3s 1ms/step - loss: 0.1159\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0891\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0876\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0881\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0880\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0880\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0880\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0877\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0881\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0879\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0875\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0879\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0876\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0879\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0876\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0875\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0881\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0874\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0876\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0882\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0872\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0875\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0877\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0875\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0873\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0878\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0871\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0875\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0872\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0877\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0870\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0872\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0873\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0874\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0877\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0873\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0873\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0877\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0872\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0871\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0873\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0871\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0872\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0868\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0874\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0875\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0870\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0869\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0872\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0869\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0872\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0873\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0871\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0871\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0869\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0872\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0865\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 2s 1ms/step - loss: 0.0872\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 2s 1ms/step - loss: 0.0870\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0868\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0871\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0868\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0872\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0867\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0871\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0871\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0871\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0868\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0867\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0865\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0867\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0865\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0861\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0871\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0868\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0863\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0864\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0867\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0870\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0866\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0869\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0867\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "Predicted Values for the test data:\n",
      "[[0.5184412 ]\n",
      " [0.4816203 ]\n",
      " [0.48103875]\n",
      " [0.51708287]\n",
      " [0.50191474]\n",
      " [0.46081322]\n",
      " [0.4661432 ]\n",
      " [0.55040187]\n",
      " [0.45851198]\n",
      " [0.4704014 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "# Generate random data\n",
    "n_samples = 1000\n",
    "n_timesteps = 1\n",
    "n_features = 4\n",
    "\n",
    "X = np.random.rand(n_samples, n_timesteps, n_features)\n",
    "y = np.random.rand(n_samples, n_timesteps)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(4, activation='tanh', input_shape=(n_timesteps, n_features)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, batch_size=1)\n",
    "\n",
    "# Generate new data for prediction (1 sample with 10 time steps and 4 features)\n",
    "X_test = np.random.rand(10, n_timesteps, n_features)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Predicted Values for the test data:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d4500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba16e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31c906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
